{
  "affirmative": {
    "precision": 0.5185185185185185,
    "recall": 0.6666666666666666,
    "f1-score": 0.5833333333333334,
    "support": 21,
    "confused_with": {
      "pick up": 3,
      "find": 2
    }
  },
  "none": {
    "precision": 0.4,
    "recall": 0.13793103448275862,
    "f1-score": 0.20512820512820515,
    "support": 29,
    "confused_with": {
      "affirmative": 8,
      "show": 6
    }
  },
  "deny": {
    "precision": 0.8823529411764706,
    "recall": 0.46875,
    "f1-score": 0.6122448979591837,
    "support": 32,
    "confused_with": {
      "pick up": 4,
      "affirmative": 4
    }
  },
  "show": {
    "precision": 0.7169811320754716,
    "recall": 0.7169811320754716,
    "f1-score": 0.7169811320754716,
    "support": 53,
    "confused_with": {
      "pick up": 6,
      "find": 6
    }
  },
  "find": {
    "precision": 0.4032258064516129,
    "recall": 0.7142857142857143,
    "f1-score": 0.5154639175257731,
    "support": 35,
    "confused_with": {
      "pick up": 6,
      "show": 2
    }
  },
  "bye": {
    "precision": 0.75,
    "recall": 0.8181818181818182,
    "f1-score": 0.7826086956521738,
    "support": 11,
    "confused_with": {
      "none": 1,
      "clarify": 1
    }
  },
  "greetings": {
    "precision": 0.75,
    "recall": 0.6428571428571429,
    "f1-score": 0.6923076923076924,
    "support": 14,
    "confused_with": {
      "pick up": 1,
      "none": 1
    }
  },
  "move": {
    "precision": 0.6896551724137931,
    "recall": 0.5405405405405406,
    "f1-score": 0.6060606060606061,
    "support": 37,
    "confused_with": {
      "pick up": 11,
      "find": 3
    }
  },
  "clarify": {
    "precision": 0.2857142857142857,
    "recall": 0.18181818181818182,
    "f1-score": 0.2222222222222222,
    "support": 33,
    "confused_with": {
      "find": 15,
      "pick up": 8
    }
  },
  "pick up": {
    "precision": 0.6495726495726496,
    "recall": 0.8,
    "f1-score": 0.7169811320754716,
    "support": 95,
    "confused_with": {
      "find": 10,
      "show": 3
    }
  },
  "accuracy": 0.6,
  "macro avg": {
    "precision": 0.6046020505922802,
    "recall": 0.5688012230908295,
    "f1-score": 0.5653331834340134,
    "support": 360
  },
  "weighted avg": {
    "precision": 0.6062286131934053,
    "recall": 0.6,
    "f1-score": 0.5833432734612252,
    "support": 360
  }
}